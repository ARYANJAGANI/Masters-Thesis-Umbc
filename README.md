# ğŸ“ Masterâ€™s Thesis â€” UMBC

**Author:** Aryan Jagani  
**Degree:** Master of Science in Information Systems  
**Institution:** University of Maryland, Baltimore County (UMBC)  
**Advisor:** Prof. Karen Chen  
**Year:** 2025â€“2026  

---

## ğŸ“˜ Thesis Title

**Evaluating Large Language Modelâ€“Generated Questions for Early Childhood Dialogic Reading**  
*(Working title â€” subject to refinement)*

---

## ğŸ“Œ Overview

This repository contains all research artifacts related to my Masterâ€™s thesis at UMBC.  
The thesis investigates the **quality, pedagogical value, and appropriateness of questions generated by Large Language Models (LLMs)** for **early childhood dialogic reading**.

The primary objective is to evaluate whether LLM-generated questions align with **established dialogic reading principles** and whether they are suitable for **young learners** in educational settings.

---

## ğŸ§  Research Focus

This thesis explores the following core questions:

- Can Large Language Models generate **educationally meaningful questions** for early childhood reading activities?
- How well do LLM-generated questions align with the **Curated Education Rubrics**?
- How do LLM-generated questions compare to **human-generated questions** across pedagogical evaluation rubrics?
- Can **human ratings and automated metrics** be used together to assess question quality reliably?

---

## ğŸ§ª Methodology (High-Level)

The research workflow includes:

1. **Book Selection**  
   - A curated set of childrenâ€™s storybooks used for early childhood reading.

2. **Question Generation**  
   - Questions generated using Large Language Models from book content.
   - Human-authored questions used as a comparison baseline.

3. **Evaluation Framework**  
   - Custom rubrics including:
     - CROWD framework alignment  
     - Answerability  
     - Relevance to text  
     - Age appropriateness  
     - Clarity and specificity  
     - Engagement potential  

4. **Evaluation Process**  
   - Human evaluation using Likert-scale ratings.
   - Automated similarity metrics (e.g., ROUGE, BLEU, BERTScore).
   - Exploration of **LLM-as-a-judge** and ML-based scoring approaches.

---

## ğŸ“‚ Repository Structure

| File / Folder | Description |
|---------------|-------------|
| `Aryan Thesis_.docx` | Full Masterâ€™s thesis document |
| `Aryan thesis Rubrics.docx` | Evaluation rubrics used in the study |
| `Next-Steps_Flowchart.docx` | Research pipeline and future work flowchart |
| `final prompt python.txt` | Prompting and scripting logic used for question generation |
| *(additional files)* | Supporting materials and analysis documents |

---

## ğŸ›  How to Use This Repository

This repository is intended for **academic reference and reproducibility**.

- ğŸ“– Read the thesis document for complete methodology and results  
- ğŸ“Š Review the rubric file to understand evaluation criteria  
- ğŸ§  Examine the prompt file to understand how LLM questions were generated  
- ğŸ” Replicate or extend the evaluation framework for future research  

> This repository serves as a **research artifact archive**, not a standalone software package.

---

## ğŸ¯ Contributions

- Proposes a **structured evaluation framework** for LLM-generated dialogic reading questions  
- Bridges **LLM evaluation** with **early childhood pedagogy**  
- Provides reusable rubrics for educational AI research  
- Highlights strengths and limitations of LLMs in early learning contexts  

---

## ğŸ“„ Citation

If you reference this work, please cite:

> Jagani, A. (2026). *Evaluating Large Language Modelâ€“Generated Questions for Early Childhood Dialogic Reading*.  
> Masterâ€™s Thesis, University of Maryland, Baltimore County.

*(Title subject to final approval.)*

---

## ğŸ“¬ Contact

**Aryan Jagani**  
GitHub: https://github.com/ARYANJAGANI  

For academic inquiries or collaboration, feel free to reach out.

---

## ğŸ·ï¸ License

This repository contains academic research materials.  
All rights reserved Â© 2026 Aryan Jagani.
